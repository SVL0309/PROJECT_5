{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "026356a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1781b71",
   "metadata": {},
   "source": [
    "#### client_id: Every client’s unique ID.\n",
    "#### variation: Indicates if a client was part of the experiment.\n",
    "#### visitor_id: A unique ID for each client-device combination.\n",
    "#### visit_id: A unique ID for each web visit/session.\n",
    "#### process_step: Marks each step in the digital process.\n",
    "#### date_time: Timestamp of each web activity.\n",
    "#### clnt_tenure_yr: Represents how long the client has been with Vanguard, measured in years.\n",
    "#### clnt_tenure_mnth: Further breaks down the client’s tenure with Vanguard in months.\n",
    "#### clnt_age: Indicates the age of the client.\n",
    "#### gendr: Specifies the client’s gender.\n",
    "#### num_accts: Denotes the number of accounts the client holds with Vanguard.\n",
    "#### bal: Gives the total balance spread across all accounts for a particular client.\n",
    "#### calls_6_mnth: Records the number of times the client reached out over a call in the past six months.\n",
    "#### logons_6_mnth: Reflects the frequency with which the client logged onto Vanguard’s platform over the last six months."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c93b972",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/f/Documents/Ironhack/PROJECT_5/df_final_demo.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df1\u001b[38;5;241m=\u001b[39mpd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/Users/f/Documents/Ironhack/PROJECT_5/df_final_demo.txt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    944\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    945\u001b[0m )\n\u001b[1;32m    946\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 948\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    608\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    610\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 611\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    613\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1448\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1445\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1447\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1448\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1705\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1703\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1704\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1705\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[1;32m   1706\u001b[0m     f,\n\u001b[1;32m   1707\u001b[0m     mode,\n\u001b[1;32m   1708\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1709\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1710\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m   1711\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[1;32m   1712\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1713\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1714\u001b[0m )\n\u001b[1;32m   1715\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1716\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/io/common.py:863\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    858\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    859\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    860\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    861\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    862\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 863\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    864\u001b[0m             handle,\n\u001b[1;32m    865\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m    866\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[1;32m    867\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[1;32m    868\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    869\u001b[0m         )\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    871\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    872\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/f/Documents/Ironhack/PROJECT_5/df_final_demo.txt'"
     ]
    }
   ],
   "source": [
    "df1=pd.read_csv('/Users/f/Documents/Ironhack/PROJECT_5/df_final_demo.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616fb330",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c4fe88",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630663cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming df is your DataFrame containing the variables\n",
    "columns_to_clean = ['clnt_tenure_yr', 'clnt_tenure_mnth', 'clnt_age', 'gendr', 'num_accts', 'bal', 'calls_6_mnth', 'logons_6_mnth']\n",
    "\n",
    "# Drop rows with missing values in the specified columns\n",
    "cleaned_df1 = df1.dropna(subset=columns_to_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8faf2a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_df1.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f71f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_df1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6bc18be",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f680d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2=pd.read_csv('/Users/f/Documents/Ironhack/PROJECT_5/df_final_experiment_clients.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668d559d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db12198",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['Variation'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be867032",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1beae9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop NaN values from the 'Variation' column\n",
    "df2_cleaned = df2.dropna(subset=['Variation'])\n",
    "\n",
    "# Count the values after dropping NaNs\n",
    "variation_counts = df2_cleaned['Variation'].value_counts()\n",
    "print(variation_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f1c465",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45bcd5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df2_cleaned filter between control and test group\n",
    "control_group = df2_cleaned[df2_cleaned['Variation'] == 'Control']\n",
    "test_group = df2_cleaned[df2_cleaned['Variation'] == 'Test']\n",
    "\n",
    "# Display the number of clients in each group\n",
    "print(\"Number of clients in Control group:\", len(control_group))\n",
    "print(\"Number of clients in Test group:\", len(test_group))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206b2c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3=pd.read_csv('/Users/f/Documents/Ironhack/PROJECT_5/df_final_web_data_pt_1.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ee11f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405672d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df4=pd.read_csv('/Users/f/Documents/Ironhack/PROJECT_5/df_final_web_data_pt_2.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3f792a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9249a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "#concat df3 and df4\n",
    "process_df = pd.concat([df3,df4],ignore_index=True)\n",
    "process_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1f184b",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63df5109",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming process_df, control_group, and test_group are your DataFrames\n",
    "\n",
    "# Check for common 'client_id' in process_df and control_group\n",
    "client_ids_control = set(control_group['client_id'])\n",
    "client_ids_control\n",
    "client_ids_test = set(test_group['client_id'])\n",
    "client_ids_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfaa3600",
   "metadata": {},
   "outputs": [],
   "source": [
    "check = 50500 - len(client_ids_control) - len(client_ids_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f78855",
   "metadata": {},
   "outputs": [],
   "source": [
    "check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac02a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "control_group = cleaned_df1[cleaned_df1['client_id'].isin(client_ids_control)]\n",
    "\n",
    "# Filter rows for test group\n",
    "test_group = cleaned_df1[cleaned_df1['client_id'].isin(client_ids_test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7494d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "control_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f0f6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b384baf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge control_group with process_df on client_id\n",
    "control_group_merged = pd.merge(control_group, process_df, on='client_id', how='inner')\n",
    "\n",
    "# Merge test_group with process_df on client_id\n",
    "test_group_merged = pd.merge(test_group, process_df, on='client_id', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93bcb168",
   "metadata": {},
   "outputs": [],
   "source": [
    "control_group_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc4e787",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_group_merged"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d132f2",
   "metadata": {},
   "source": [
    "#### Hypothesis:\n",
    "#### User Engagement Hypothesis:\n",
    "#### Null Hypothesis (H0): There is no difference in user engagement metrics (such as calls_6_mnth and logons_6_mnth) between the control and test groups.\n",
    "#### Alternative Hypothesis (H1): Users in the test group have higher user engagement metrics compared to users in the control group.\n",
    "\n",
    "#### User Satisfaction Hypothesis:\n",
    "#### Null Hypothesis (H0): There is no difference in user satisfaction (potentially inferred from num_accts and bal) between the control and test groups.\n",
    "#### Alternative Hypothesis (H1): Users in the test group are more satisfied with the platform compared to users in the control group.\n",
    "\n",
    "#### User Experience Hypothesis:\n",
    "#### Null Hypothesis (H0): There is no difference in user experience (potentially inferred from visit_id and process_step) between the control and test groups.\n",
    "#### Alternative Hypothesis (H1): Users in the test group have a better user experience with the platform compared to users in the control group.\n",
    "\n",
    "#### Demographic Differences Hypothesis:\n",
    "#### Null Hypothesis (H0): There is no difference in demographic characteristics (such as clnt_age and gendr) between the control and test groups.\n",
    "#### Alternative Hypothesis (H1): There are demographic differences between the control and test groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13562b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the values or categories within each variable\n",
    "print(\"Values or Categories in the 'client_id' Variable:\")\n",
    "print(test_group_merged['client_id'].unique())\n",
    "\n",
    "print(\"\\nValues or Categories in the 'visitor_id' Variable:\")\n",
    "print(test_group_merged['visitor_id'].unique())\n",
    "\n",
    "print(\"\\nValues or Categories in the 'visit_id' Variable:\")\n",
    "print(test_group_merged['visit_id'].unique())\n",
    "\n",
    "print(\"\\nValues or Categories in the 'process_step' Variable:\")\n",
    "print(test_group_merged['process_step'].unique())\n",
    "\n",
    "print(\"\\nValues or Categories in the 'date_time' Variable:\")\n",
    "print(test_group_merged['date_time'].unique())\n",
    "\n",
    "print(\"\\nValues or Categories in the 'clnt_tenure_yr' Variable:\")\n",
    "print(test_group_merged['clnt_tenure_yr'].unique())\n",
    "\n",
    "print(\"\\nValues or Categories in the 'clnt_tenure_mnth' Variable:\")\n",
    "print(test_group_merged['clnt_tenure_mnth'].unique())\n",
    "\n",
    "print(\"\\nValues or Categories in the 'clnt_age' Variable:\")\n",
    "print(test_group_merged['clnt_age'].unique())\n",
    "\n",
    "print(\"\\nValues or Categories in the 'gendr' Variable:\")\n",
    "print(test_group_merged['gendr'].unique())\n",
    "\n",
    "print(\"\\nValues or Categories in the 'num_accts' Variable:\")\n",
    "print(test_group_merged['num_accts'].unique())\n",
    "\n",
    "print(\"\\nValues or Categories in the 'bal' Variable:\")\n",
    "print(test_group_merged['bal'].unique())\n",
    "\n",
    "print(\"\\nValues or Categories in the 'calls_6_mnth' Variable:\")\n",
    "print(test_group_merged['calls_6_mnth'].unique())\n",
    "\n",
    "print(\"\\nValues or Categories in the 'logons_6_mnth' Variable:\")\n",
    "print(test_group_merged['logons_6_mnth'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4ab27c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Get the top 10 unique values or categories for each variable in the test_group_merged DataFrame\n",
    "print(\"Top 10 Values or Categories in the 'client_id' Variable:\")\n",
    "print(test_group_merged['client_id'].value_counts().head(10))\n",
    "\n",
    "print(\"\\nTop 10 Values or Categories in the 'visitor_id' Variable:\")\n",
    "print(test_group_merged['visitor_id'].value_counts().head(10))\n",
    "\n",
    "print(\"\\nTop 10 Values or Categories in the 'visit_id' Variable:\")\n",
    "print(test_group_merged['visit_id'].value_counts().head(10))\n",
    "\n",
    "print(\"\\nTop 10 Values or Categories in the 'process_step' Variable:\")\n",
    "print(test_group_merged['process_step'].value_counts().head(10))\n",
    "\n",
    "print(\"\\nTop 10 Values or Categories in the 'date_time' Variable:\")\n",
    "print(test_group_merged['date_time'].value_counts().head(10))\n",
    "\n",
    "print(\"\\nTop 10 Values or Categories in the 'clnt_tenure_yr' Variable:\")\n",
    "print(test_group_merged['clnt_tenure_yr'].value_counts().head(10))\n",
    "\n",
    "print(\"\\nTop 10 Values or Categories in the 'clnt_tenure_mnth' Variable:\")\n",
    "print(test_group_merged['clnt_tenure_mnth'].value_counts().head(10))\n",
    "\n",
    "print(\"\\nTop 10 Values or Categories in the 'clnt_age' Variable:\")\n",
    "print(test_group_merged['clnt_age'].value_counts().head(10))\n",
    "\n",
    "print(\"\\nTop 10 Values or Categories in the 'gendr' Variable:\")\n",
    "print(test_group_merged['gendr'].value_counts().head(10))\n",
    "\n",
    "print(\"\\nTop 10 Values or Categories in the 'num_accts' Variable:\")\n",
    "print(test_group_merged['num_accts'].value_counts().head(10))\n",
    "\n",
    "print(\"\\nTop 10 Values or Categories in the 'bal' Variable:\")\n",
    "print(test_group_merged['bal'].value_counts().head(10))\n",
    "\n",
    "print(\"\\nTop 10 Values or Categories in the 'calls_6_mnth' Variable:\")\n",
    "print(test_group_merged['calls_6_mnth'].value_counts().head(10))\n",
    "\n",
    "print(\"\\nTop 10 Values or Categories in the 'logons_6_mnth' Variable:\")\n",
    "print(test_group_merged['logons_6_mnth'].value_counts().head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1079f5f9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Get the top 10 unique values or categories for each variable in the control_group_merged DataFrame\n",
    "print(\"Top 10 Values or Categories in the 'client_id' Variable:\")\n",
    "print(control_group_merged['client_id'].value_counts().head(10))\n",
    "\n",
    "print(\"\\nTop 10 Values or Categories in the 'visitor_id' Variable:\")\n",
    "print(control_group_merged['visitor_id'].value_counts().head(10))\n",
    "\n",
    "print(\"\\nTop 10 Values or Categories in the 'visit_id' Variable:\")\n",
    "print(control_group_merged['visit_id'].value_counts().head(10))\n",
    "\n",
    "print(\"\\nTop 10 Values or Categories in the 'process_step' Variable:\")\n",
    "print(control_group_merged['process_step'].value_counts().head(10))\n",
    "\n",
    "print(\"\\nTop 10 Values or Categories in the 'date_time' Variable:\")\n",
    "print(control_group_merged['date_time'].value_counts().head(10))\n",
    "\n",
    "print(\"\\nTop 10 Values or Categories in the 'clnt_tenure_yr' Variable:\")\n",
    "print(control_group_merged['clnt_tenure_yr'].value_counts().head(10))\n",
    "\n",
    "print(\"\\nTop 10 Values or Categories in the 'clnt_tenure_mnth' Variable:\")\n",
    "print(control_group_merged['clnt_tenure_mnth'].value_counts().head(10))\n",
    "\n",
    "print(\"\\nTop 10 Values or Categories in the 'clnt_age' Variable:\")\n",
    "print(control_group_merged['clnt_age'].value_counts().head(10))\n",
    "\n",
    "print(\"\\nTop 10 Values or Categories in the 'gendr' Variable:\")\n",
    "print(control_group_merged['gendr'].value_counts().head(10))\n",
    "\n",
    "print(\"\\nTop 10 Values or Categories in the 'num_accts' Variable:\")\n",
    "print(control_group_merged['num_accts'].value_counts().head(10))\n",
    "\n",
    "print(\"\\nTop 10 Values or Categories in the 'bal' Variable:\")\n",
    "print(control_group_merged['bal'].value_counts().head(10))\n",
    "\n",
    "print(\"\\nTop 10 Values or Categories in the 'calls_6_mnth' Variable:\")\n",
    "print(control_group_merged['calls_6_mnth'].value_counts().head(10))\n",
    "\n",
    "print(\"\\nTop 10 Values or Categories in the 'logons_6_mnth' Variable:\")\n",
    "print(control_group_merged['logons_6_mnth'].value_counts().head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16de2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming control_group_merged is your DataFrame containing the 'gendr' column\n",
    "# Calculate the total count of 'U' gender category\n",
    "total_U_count = control_group_merged['gendr'].value_counts()['U']\n",
    "\n",
    "# Calculate the count for each gender after splitting\n",
    "count_M = total_U_count // 2\n",
    "count_F = total_U_count - count_M\n",
    "\n",
    "# Replace half of the 'U' counts with 'M' and the other half with 'F'\n",
    "control_group_merged.loc[control_group_merged['gendr'] == 'U', 'gendr'] = ['M'] * count_M + ['F'] * count_F\n",
    "\n",
    "# Verify the result\n",
    "print(control_group_merged['gendr'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617a4aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming test_group is your DataFrame containing the 'gendr' column\n",
    "# Drop the 'X' category\n",
    "test_group_merged = test_group_merged[test_group_merged['gendr'] != 'X']\n",
    "\n",
    "# Calculate the total count of 'U' gender category\n",
    "total_U_count = test_group_merged['gendr'].value_counts()['U']\n",
    "\n",
    "# Calculate the count for each gender after splitting\n",
    "count_M = total_U_count // 2\n",
    "count_F = total_U_count - count_M\n",
    "\n",
    "# Replace half of the 'U' counts with 'M' and the other half with 'F'\n",
    "test_group_merged.loc[test_group_merged['gendr'] == 'U', 'gendr'] = ['M'] * count_M + ['F'] * count_F\n",
    "\n",
    "# Verify the result\n",
    "print(test_group_merged['gendr'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de918b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_group_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e77a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the bin edges and labels for age categories\n",
    "bin_edges = [0, 18, 30, 40, 50, 60, 70, 80, 90, 100]\n",
    "bin_labels = ['0-18', '19-30', '31-40', '41-50', '51-60', '61-70', '71-80', '81-90', '91-100']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418476ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the 'age_category' column using pd.cut\n",
    "test_group_merged['age_category'] = pd.cut(test_group_merged['clnt_age'], bins=bin_edges, labels=bin_labels)\n",
    "\n",
    "# Display the count of each age category\n",
    "print(test_group_merged['age_category'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19615fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the 'age_category' column using pd.cut\n",
    "control_group_merged['age_category'] = pd.cut(control_group_merged['clnt_age'], bins=bin_edges, labels=bin_labels)\n",
    "\n",
    "# Display the count of each age category\n",
    "print(control_group_merged['age_category'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65350cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "control_group_merged['clnt_tenure_yr'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd3f30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the bin edges and labels for tenure categories\n",
    "bin_edges = [2, 10, 20, 30, 40, 50, 60]\n",
    "bin_labels = ['2-10', '11-20', '21-30', '31-40', '41-50', '51-60']\n",
    "\n",
    "# Create the 'tenure_category' column using pd.cut\n",
    "control_group_merged['tenure_category'] = pd.cut(control_group_merged['clnt_tenure_yr'], bins=bin_edges, labels=bin_labels)\n",
    "\n",
    "# Display the DataFrame with the new 'tenure_category' column\n",
    "print(control_group_merged[['clnt_tenure_yr', 'tenure_category']])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ebb050f",
   "metadata": {},
   "outputs": [],
   "source": [
    "control_group_merged['tenure_category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb56950",
   "metadata": {},
   "outputs": [],
   "source": [
    "control_group_merged['bal'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa155dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the bin edges and labels for balance categories\n",
    "bin_edges = [2.378961e+04, 1e+06, 2e+06, 3e+06, 4e+06, 5e+06, 6e+06, 7e+06, 8e+06, 8.292996e+06]\n",
    "bin_labels = ['23,789 - 1M', '1M - 2M', '2M - 3M', '3M - 4M', '4M - 5M', '5M - 6M', '6M - 7M', '7M - 8M', '8M - 8.29M']\n",
    "\n",
    "# Create the 'balance_category' column using pd.cut\n",
    "control_group_merged['bal_category'] = pd.cut(control_group_merged['bal'], bins=bin_edges, labels=bin_labels)\n",
    "\n",
    "# Display the DataFrame with the new 'balance_category' column\n",
    "print(control_group_merged[['bal', 'bal_category']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f52c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "control_group_merged['bal_category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14812469",
   "metadata": {},
   "outputs": [],
   "source": [
    "control_group_merged['num_accts'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5fcc1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the bin edges and labels for number of accounts categories\n",
    "bin_edges = [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0]\n",
    "bin_labels = ['1-2', '2-3', '3-4', '4-5', '5-6', '6-7']\n",
    "\n",
    "# Create the 'num_accts_category' column using pd.cut\n",
    "control_group_merged['num_accts_category'] = pd.cut(control_group_merged['num_accts'], bins=bin_edges, labels=bin_labels)\n",
    "\n",
    "# Display the DataFrame with the new 'num_accts_category' column\n",
    "print(control_group_merged[['num_accts', 'num_accts_category']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b43e13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "control_group_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb65480",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get statistics for the 'bal' variable\n",
    "test_group_merged['bal'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1eabc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the bin edges and labels for balance categories\n",
    "bin_edges = [2.378944e+04, 1e+06, 2e+06, 3e+06, 4e+06, 5e+06, 6e+06, 7e+06, 8e+06, 8.292996e+06]\n",
    "bin_labels = ['23,789 - 1M', '1M - 2M', '2M - 3M', '3M - 4M', '4M - 5M', '5M - 6M', '6M - 7M', '7M - 8M', '8M - 8.29M']\n",
    "\n",
    "# Create the 'bal_category' column using pd.cut\n",
    "test_group_merged['bal_category'] = pd.cut(test_group_merged['bal'], bins=bin_edges, labels=bin_labels)\n",
    "\n",
    "# Display the DataFrame with the new 'bal_category' column\n",
    "print(test_group_merged[['bal', 'bal_category']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf615265",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get statistics for the 'bal' variable\n",
    "test_group_merged['clnt_tenure_yr'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a344ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the bin edges and labels for tenure categories\n",
    "bin_edges = [2.0, 10.0, 20.0, 30.0, 40.0, 50.0, 55.0]\n",
    "bin_labels = ['2-10', '11-20', '21-30', '31-40', '41-50', '51-55']\n",
    "\n",
    "# Create the 'clnt_tenure_category' column using pd.cut\n",
    "test_group_merged['clnt_tenure_category'] = pd.cut(test_group_merged['clnt_tenure_yr'], bins=bin_edges, labels=bin_labels)\n",
    "\n",
    "# Display the DataFrame with the new 'clnt_tenure_category' column\n",
    "print(test_group_merged[['clnt_tenure_yr', 'clnt_tenure_category']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87cf8548",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get statistics for the 'bal' variable\n",
    "test_group_merged['num_accts'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3210bb13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the bin edges and labels for number of accounts categories\n",
    "bin_edges = [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0]\n",
    "bin_labels = ['1-2', '2-3', '3-4', '4-5', '5-6', '6-7']\n",
    "\n",
    "# Create the 'num_accts_category' column using pd.cut\n",
    "test_group_merged['num_accts_category'] = pd.cut(test_group_merged['num_accts'], bins=bin_edges, labels=bin_labels)\n",
    "\n",
    "# Display the DataFrame with the new 'num_accts_category' column\n",
    "print(test_group_merged[['num_accts', 'num_accts_category']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6fd9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answered the following questions about demographics:\n",
    "#Who are the primary clients using this online process?\n",
    "#Are the primary clients younger or older, new or long-standing?\n",
    "test_group_merged"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3e26ad",
   "metadata": {},
   "source": [
    "Day 2 (Week 5)\n",
    "\n",
    "For the project requirements and instructions for today’s tasks in full, please refer to the project brief. However, in order to keep on track you may refer to the daily goals outlined below:\n",
    "\n",
    "By the end of the first two days, we recommend you have:\n",
    "\n",
    "Done dataset discovery: Understood the nature and structure of your datasets using Python with libraries such as Pandas, Matplotlib, and Seaborn. Carried out data cleaning and fix any problems if there are any. Answered the following questions about demographics: Who are the primary clients using this online process? Are the primary clients younger or older, new or long-standing? Carried out a client behaviour analysis to answer any additional relevant questions you think are important.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfdeddf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set the style of the seaborn plot\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Create subplots for each demographic variable\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "\n",
    "# Gender distribution\n",
    "sns.countplot(data=test_group_merged, x='gendr', ax=axes[0, 0])\n",
    "axes[0, 0].set_title('Gender Distribution')\n",
    "\n",
    "# Age category distribution\n",
    "sns.countplot(data=test_group_merged, x='age_category', ax=axes[0, 1])\n",
    "axes[0, 1].set_title('Age Category Distribution')\n",
    "\n",
    "# Balance category distribution\n",
    "sns.countplot(data=test_group_merged, x='bal_category', ax=axes[0, 2])\n",
    "axes[0, 2].set_title('Balance Category Distribution')\n",
    "\n",
    "# Client tenure category distribution\n",
    "sns.countplot(data=test_group_merged, x='clnt_tenure_category', ax=axes[1, 0])\n",
    "axes[1, 0].set_title('Client Tenure Category Distribution')\n",
    "\n",
    "# Number of accounts category distribution\n",
    "sns.countplot(data=test_group_merged, x='num_accts_category', ax=axes[1, 1])\n",
    "axes[1, 1].set_title('Number of Accounts Category Distribution')\n",
    "\n",
    "# Hide the empty subplot\n",
    "fig.delaxes(axes[1, 2])\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8fd7e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "control_group_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc897bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set the style of the seaborn plot\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Create subplots for each demographic variable\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "\n",
    "# Gender distribution\n",
    "sns.countplot(data=control_group_merged, x='gendr', ax=axes[0, 0])\n",
    "axes[0, 0].set_title('Gender Distribution')\n",
    "\n",
    "# Age category distribution\n",
    "sns.countplot(data=control_group_merged, x='age_category', ax=axes[0, 1])\n",
    "axes[0, 1].set_title('Age Category Distribution')\n",
    "\n",
    "# Balance category distribution\n",
    "sns.countplot(data=control_group_merged, x='bal_category', ax=axes[0, 2])\n",
    "axes[0, 2].set_title('Balance Category Distribution')\n",
    "\n",
    "# Client tenure category distribution\n",
    "sns.countplot(data=control_group_merged, x='tenure_category', ax=axes[1, 0])\n",
    "axes[1, 0].set_title('Client Tenure Category Distribution')\n",
    "\n",
    "# Number of accounts category distribution\n",
    "sns.countplot(data=control_group_merged, x='num_accts_category', ax=axes[1, 1])\n",
    "axes[1, 1].set_title('Number of Accounts Category Distribution')\n",
    "\n",
    "# Hide the empty subplot\n",
    "fig.delaxes(axes[1, 2])\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plots\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1f2dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze client demographics for control group\n",
    "primary_clients_control = control_group_merged.groupby(['age_category', 'gendr', 'clnt_tenure_yr', 'num_accts', 'bal']).size().reset_index(name='count')\n",
    "\n",
    "# Sort by count to identify primary clients\n",
    "primary_clients_control_sorted = primary_clients_control.sort_values(by='count', ascending=False)\n",
    "\n",
    "# Output the results\n",
    "print(\"Primary Clients in Control Group:\")\n",
    "print(primary_clients_control_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a330845b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set the style of the seaborn plot\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Create the bar plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "barplot = sns.barplot(data=primary_clients_control_sorted, x='age_category', y='count', hue='gendr', ci=None)\n",
    "\n",
    "# Add count labels to each bar\n",
    "for p in barplot.patches:\n",
    "    barplot.annotate(format(p.get_height(), '.0f'), \n",
    "                     (p.get_x() + p.get_width() / 2., p.get_height()), \n",
    "                     ha = 'center', va = 'center', \n",
    "                     xytext = (0, 9), \n",
    "                     textcoords = 'offset points')\n",
    "\n",
    "plt.title('Primary Clients in Control Group by Age Category and Gender')\n",
    "plt.xlabel('Age Category')\n",
    "plt.ylabel('Count')\n",
    "plt.legend(title='Gender')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c5aac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze client demographics for control group\n",
    "primary_clients_test = test_group_merged.groupby(['age_category', 'gendr', 'clnt_tenure_yr', 'num_accts', 'bal']).size().reset_index(name='count')\n",
    "\n",
    "# Sort by count to identify primary clients\n",
    "primary_clients_test_sorted = primary_clients_test.sort_values(by='count', ascending=False)\n",
    "\n",
    "# Output the results\n",
    "print(\"Primary Clients in Control Group:\")\n",
    "print(primary_clients_test_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c66f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set the style of the seaborn plot\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Create the bar plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "barplot = sns.barplot(data=primary_clients_test_sorted, x='age_category', y='count', hue='gendr', ci=None)\n",
    "\n",
    "# Add count labels to each bar\n",
    "for p in barplot.patches:\n",
    "    barplot.annotate(format(p.get_height(), '.0f'), \n",
    "                     (p.get_x() + p.get_width() / 2., p.get_height()), \n",
    "                     ha = 'center', va = 'center', \n",
    "                     xytext = (0, 9), \n",
    "                     textcoords = 'offset points')\n",
    "\n",
    "plt.title('Primary Clients in Test Group by Age Category and Gender')\n",
    "plt.xlabel('Age Category')\n",
    "plt.ylabel('Count')\n",
    "plt.legend(title='Gender')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce445585",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate the mean of clnt_tenure_yr for each age_category group\n",
    "average_tenure_by_age = primary_clients_test_sorted.groupby('age_category')['clnt_tenure_yr'].mean().reset_index()\n",
    "\n",
    "# Set the style of the seaborn plot\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Create the bar plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "barplot = sns.barplot(data=average_tenure_by_age, x='age_category', y='clnt_tenure_yr', ci=None)\n",
    "\n",
    "# Add count labels to each bar\n",
    "for p in barplot.patches:\n",
    "    barplot.annotate(format(p.get_height(), '.2f'), \n",
    "                     (p.get_x() + p.get_width() / 2., p.get_height()), \n",
    "                     ha='center', va='center', \n",
    "                     xytext=(0, 9), \n",
    "                     textcoords='offset points')\n",
    "\n",
    "plt.title('Average Client Tenure Year by Age Category')\n",
    "plt.xlabel('Age Category')\n",
    "plt.ylabel('Average Client Tenure Year')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74186c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_by_client(df):\n",
    "    \"\"\"\n",
    "    Groups the data by clients and separates steps and times into different columns.\n",
    "    \n",
    "    Parameters:\n",
    "    df : pandas.DataFrame\n",
    "        DataFrame with data containing columns 'client_id', 'process_step', and 'date_time'.\n",
    "    \n",
    "    Returns:\n",
    "    pandas.DataFrame\n",
    "        DataFrame where data is grouped by clients, and steps and times are in separate columns.\n",
    "    \"\"\"\n",
    "    # Group the data by clients\n",
    "    grouped_df = df.groupby(\"client_id\")\n",
    "    \n",
    "    # Create a dictionary to store steps and times for each client\n",
    "    data_dict = {\"client_id\": [], \"step_1\": [], \"time_1\": [], \"step_2\": [], \"time_2\": [],\n",
    "                 \"step_3\": [], \"time_3\": [], \"step_4\": [], \"time_4\": [], \"step_5\": [], \"time_5\": []}\n",
    "    \n",
    "    # For each client, extract their steps and times and add them to the dictionary\n",
    "    for client_id, group_data in grouped_df:\n",
    "        # Get the steps and times for the current client\n",
    "        steps = group_data[\"process_step\"].tolist()[::-1]  # Reverse the list to start from the last step\n",
    "        times = group_data[\"date_time\"].tolist()[::-1]    # Reverse the list to match the reversed steps\n",
    "        \n",
    "        # Fill in the dictionary with steps and times\n",
    "        for i in range(5):  # Assuming maximum of 5 steps\n",
    "            if i < len(steps):\n",
    "                data_dict[f\"step_{i+1}\"].append(steps[i])\n",
    "                data_dict[f\"time_{i+1}\"].append(times[i])\n",
    "            else:\n",
    "                # If there are fewer than 5 steps, fill in with NaN\n",
    "                data_dict[f\"step_{i+1}\"].append(None)\n",
    "                data_dict[f\"time_{i+1}\"].append(None)\n",
    "        \n",
    "        # Add client_id to the dictionary\n",
    "        data_dict[\"client_id\"].append(client_id)\n",
    "    \n",
    "    # Create a DataFrame from the dictionary\n",
    "    result_df = pd.DataFrame(data_dict)\n",
    "    \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a61c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_df_with_steps = group_by_client(process_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04d4e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_df_with_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9846e807",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_by_client_start_end(df):\n",
    "    \"\"\"\n",
    "    Groups the data by clients and includes the first and last steps made by each client,\n",
    "    along with their start and end times.\n",
    "    \n",
    "    Parameters:\n",
    "    df : pandas.DataFrame\n",
    "        DataFrame with data containing columns 'client_id', 'process_step', and 'date_time'.\n",
    "    \n",
    "    Returns:\n",
    "    pandas.DataFrame\n",
    "        DataFrame where data is grouped by clients, including the first and last steps,\n",
    "        the start time of the first step, and the end time of the last step.\n",
    "    \"\"\"\n",
    "    # Convert 'date_time' column to datetime format\n",
    "    df['date_time'] = pd.to_datetime(df['date_time'])\n",
    "    \n",
    "    # Sort the dataframe by 'date_time' within each group\n",
    "    df_sorted = df.sort_values(by=['client_id', 'date_time'])\n",
    "    \n",
    "    # Group the data by clients\n",
    "    grouped_df = df_sorted.groupby(\"client_id\")\n",
    "    \n",
    "    # Create a dictionary to store the data for each client\n",
    "    data_dict = {\"client_id\": [], \"start_step\": [], \"end_step\": [], \"start_time\": [], \"end_time\": []}\n",
    "    \n",
    "    # For each client, extract the first and last steps and their start and end times\n",
    "    for client_id, group_data in grouped_df:\n",
    "        # Get the first step\n",
    "        start_step = group_data[\"process_step\"].iloc[0]\n",
    "        \n",
    "        # Get the last step\n",
    "        end_step = group_data[\"process_step\"].iloc[-1]\n",
    "        \n",
    "        # Get the start time of the first step\n",
    "        start_time = group_data[\"date_time\"].iloc[0]\n",
    "        \n",
    "        # Get the end time of the last step\n",
    "        end_time = group_data[\"date_time\"].iloc[-1]\n",
    "        \n",
    "        # Add the data to the dictionary\n",
    "        data_dict[\"client_id\"].append(client_id)\n",
    "        data_dict[\"start_step\"].append(start_step)\n",
    "        data_dict[\"end_step\"].append(end_step)\n",
    "        data_dict[\"start_time\"].append(start_time)\n",
    "        data_dict[\"end_time\"].append(end_time)\n",
    "    \n",
    "    # Create a DataFrame from the dictionary\n",
    "    result_df = pd.DataFrame(data_dict)\n",
    "    \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc015106",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_df_by_client_last_step = group_by_client_start_end(process_df)\n",
    "process_df_by_client_last_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991875a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_df_by_client_last_step['end_step'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52972231",
   "metadata": {},
   "outputs": [],
   "source": [
    "control_group_with_steps = pd.merge(control_group, process_df_by_client_last_step, on=\"client_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e33ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_group_with_steps = pd.merge(test_group, process_df_by_client_last_step, on=\"client_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a3ef14",
   "metadata": {},
   "outputs": [],
   "source": [
    "control_group_with_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4c7836",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_group_with_steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac392f44",
   "metadata": {},
   "source": [
    "Day 3 (Week 5)\n",
    "\n",
    "For the project requirements and instructions for today’s tasks in full, please refer to the project brief. However, in order to keep on track you may refer to the daily goals outlined below:\n",
    "\n",
    "By the end of day, we recommend you have:\n",
    "\n",
    "Reviewed KPI and Metrics material. Discovered what key performance indicators (KPIs) will determine the success of the new design Use at least completion rate, time spent on each step and error rates. Add any KPIs you might find relevant. Evaluated how the new design’s performance compare to the old one, given the chosen KPIs (completion rate, time spent on each step and error rates)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76821d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Step 1: Define Key Performance Indicators (KPIs)\n",
    "\n",
    "# Step 2: Data Analysis\n",
    "\n",
    "## Completion Rate Analysis\n",
    "old_completion_rate = (control_group_with_steps['end_step'] == 'confirm').mean()\n",
    "new_completion_rate = (test_group_with_steps['end_step'] == 'confirm').mean()\n",
    "\n",
    "## Time Spent Analysis\n",
    "old_time_spent = control_group_with_steps.groupby('client_id').apply(lambda x: x['end_time'].max() - x['start_time'].min())\n",
    "new_time_spent = test_group_with_steps.groupby('client_id').apply(lambda x: x['end_time'].max() - x['start_time'].min())\n",
    "\n",
    "## Error Rate Analysis\n",
    "old_errors = control_group_with_steps[control_group_with_steps['end_step'] != 'confirm']\n",
    "new_errors = test_group_with_steps[test_group_with_steps['end_step'] != 'confirm']\n",
    "\n",
    "old_error_rate = len(old_errors) / len(control_group_with_steps)\n",
    "new_error_rate = len(new_errors) / len(test_group_with_steps)\n",
    "\n",
    "# Step 3: Visualization and Reporting\n",
    "\n",
    "## Completion Rate Visualization\n",
    "plt.bar(['Old Design', 'New Design'], [old_completion_rate, new_completion_rate])\n",
    "plt.title('Completion Rate Comparison')\n",
    "plt.xlabel('Design')\n",
    "plt.ylabel('Completion Rate')\n",
    "plt.show()\n",
    "\n",
    "## Time Spent Visualization\n",
    "plt.boxplot([old_time_spent.dt.seconds/60, new_time_spent.dt.seconds/60], labels=['Old Design', 'New Design'])\n",
    "plt.title('Time Spent on Process Comparison')\n",
    "plt.xlabel('Design')\n",
    "plt.ylabel('Time Spent (minutes)')\n",
    "plt.show()\n",
    "\n",
    "## Error Rate Visualization\n",
    "plt.bar(['Old Design', 'New Design'], [old_error_rate, new_error_rate])\n",
    "plt.title('Error Rate Comparison')\n",
    "plt.xlabel('Design')\n",
    "plt.ylabel('Error Rate')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a84bfdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate step percentages\n",
    "def calculate_step_percentages(data):\n",
    "    step_counts = data['end_step'].value_counts(normalize=True) * 100\n",
    "    return step_counts\n",
    "\n",
    "## Step Analysis Visualization\n",
    "old_step_percentages = calculate_step_percentages(control_group_with_steps)\n",
    "new_step_percentages = calculate_step_percentages(test_group_with_steps)\n",
    "\n",
    "fig, axes = plt.subplots(2, 1, figsize=(10, 8))\n",
    "\n",
    "old_step_percentages.plot(kind='bar', ax=axes[0], color='skyblue')\n",
    "new_step_percentages.plot(kind='bar', ax=axes[1], color='orange')\n",
    "axes[0].set_title('Old Design Step Percentages')\n",
    "axes[1].set_title('New Design Step Percentages')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11eac450",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the step percentages dataframes along the columns axis\n",
    "combined_step_percentages = pd.concat([old_step_percentages, new_step_percentages], axis=1)\n",
    "\n",
    "# Plot the combined step percentages\n",
    "combined_step_percentages.plot(kind='bar', figsize=(10, 6), color=['skyblue', 'orange'])\n",
    "plt.title('Step Percentages Comparison')\n",
    "plt.xlabel('Steps')\n",
    "plt.ylabel('Percentage')\n",
    "plt.legend(['Old Design', 'New Design'])\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ac7de7",
   "metadata": {},
   "source": [
    "Day 4 (Week 5) For the project requirements and instructions for today’s tasks in full, please refer to the project brief. However, in order to keep on track you may refer to the daily goals outlined below:\n",
    "\n",
    "By the end of both day 4 and day 5, we recommend you have:\n",
    "\n",
    "Confirmed if the difference in completion rate of the the new design and the old design is statistically significant. Carried out an analysis ensuring that the observed increase in completion rate from the A/B test meets or exceeds this 5% threshold. Carried out another hypothesis test of your choosing. Evaluated the experiment by answering questions relating to: Design Effectiveness Duration Additional Data Needs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3a9c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Formulate Hypotheses:\n",
    "#Null Hypothesis (H0): There is no difference in completion rates between the control group (old design) and the test group (new design).\n",
    "#H0: 𝜇_old = 𝜇_new\n",
    "#Alternative Hypothesis (H1): There is a difference in completion rates between the control group (old design) and the test group (new design).\n",
    "#H1: 𝜇_old ≠ 𝜇_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010a32ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Choose Significance Level (Alpha):\n",
    "#Determine the significance level (alpha) for the test.\n",
    "alpha = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002e2679",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "\n",
    "def hypothesis_test(control_data, test_data, alpha=0.05):\n",
    "    # Step 1: Formulate Hypotheses\n",
    "    # Null Hypothesis (H0): 𝜇_old = 𝜇_new\n",
    "    # Alternative Hypothesis (H1): 𝜇_old ≠ 𝜇_new\n",
    "    \n",
    "    # Step 2: Choose Significance Level (Alpha)\n",
    "    # Alpha is already provided as a parameter\n",
    "    \n",
    "    # Step 3: Calculate Test Statistic\n",
    "    t_statistic, p_value = stats.ttest_ind(control_data, test_data)\n",
    "    \n",
    "    # Step 4: Determine Degrees of Freedom\n",
    "    df = len(control_data) + len(test_data) - 2\n",
    "    \n",
    "    # Step 5: Calculate p-value\n",
    "    # For a two-tailed test, multiply the p-value by 2\n",
    "    p_value *= 2\n",
    "    \n",
    "    # Step 6: Make Decision\n",
    "    if p_value < alpha:\n",
    "        decision = \"Reject the null hypothesis\"\n",
    "    else:\n",
    "        decision = \"Fail to reject the null hypothesis\"\n",
    "    \n",
    "    return t_statistic, df, p_value, decision\n",
    "\n",
    "# Example usage:\n",
    "control_data = control_group_with_steps['end_step'] == 'confirm'  # Completion rates for control group\n",
    "test_data = test_group_with_steps['end_step'] == 'confirm'        # Completion rates for test group\n",
    "\n",
    "t_statistic, df, p_value, decision = hypothesis_test(control_data, test_data)\n",
    "\n",
    "print(\"Test Statistic:\", t_statistic)\n",
    "print(\"Degrees of Freedom:\", df)\n",
    "print(\"p-value:\", p_value)\n",
    "print(\"Decision:\", decision)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ecc0cea",
   "metadata": {},
   "source": [
    "This output suggests that there is a significant difference between the completion rates of the old design (control group) and the new design (test group). \n",
    "\n",
    "- Test Statistic: The test statistic is a measure of how much the completion rates differ between the two groups. In this case, the negative value indicates that the completion rates are lower in the new design compared to the old design.\n",
    "\n",
    "- Degrees of Freedom: This represents the number of independent pieces of information available to estimate a statistic. In this context, it indicates the number of data points used to calculate the test statistic.\n",
    "\n",
    "- p-value: The p-value is the probability of observing a test statistic as extreme as the one calculated if the null hypothesis (that there is no difference between the completion rates of the old and new designs) were true. A very small p-value (in this case, close to 0) suggests that the observed difference is unlikely to be due to random chance.\n",
    "\n",
    "- Decision: Based on the very small p-value (4.18804279000344e-103), we reject the null hypothesis. This means that we have evidence to conclude that there is a significant difference in completion rates between the old and new designs. Specifically, the completion rates are significantly lower in the new design compared to the old design."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025b7a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "\n",
    "def hypothesis_test(control_data, test_data, alpha=0.05):\n",
    "    # Step 1: Formulate Hypotheses\n",
    "    # Null Hypothesis (H0): There is no association between the old and new designs\n",
    "    # Alternative Hypothesis (H1): There is an association between the old and new designs\n",
    "    \n",
    "    # Step 2: Choose Significance Level (Alpha)\n",
    "    # Alpha is already provided as a parameter\n",
    "    \n",
    "    # Step 3: Create Contingency Table\n",
    "    contingency_table = pd.crosstab(control_data, test_data)\n",
    "    \n",
    "    # Step 4: Calculate Test Statistic and p-value\n",
    "    chi2_statistic, p_value, _, _ = stats.chi2_contingency(contingency_table)\n",
    "    \n",
    "    # Step 5: Determine Degrees of Freedom\n",
    "    df = (contingency_table.shape[0] - 1) * (contingency_table.shape[1] - 1)\n",
    "    \n",
    "    # Step 6: Make Decision\n",
    "    if p_value < alpha:\n",
    "        decision = \"Reject the null hypothesis\"\n",
    "    else:\n",
    "        decision = \"Fail to reject the null hypothesis\"\n",
    "    \n",
    "    return chi2_statistic, df, p_value, decision\n",
    "\n",
    "# Example usage:\n",
    "control_data = control_group_with_steps['end_step'] == 'confirm'  # Completion rates for control group\n",
    "test_data = test_group_with_steps['end_step'] == 'confirm'        # Completion rates for test group\n",
    "\n",
    "chi2_statistic, df, p_value, decision = hypothesis_test(control_data, test_data)\n",
    "\n",
    "print(\"Chi-Square Statistic:\", chi2_statistic)\n",
    "print(\"Degrees of Freedom:\", df)\n",
    "print(\"p-value:\", p_value)\n",
    "print(\"Decision:\", decision)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4773a3f",
   "metadata": {},
   "source": [
    "This result indicates that there is insufficient evidence to reject the null hypothesis. In other words, we do not have enough evidence to conclude that there is a significant association between the old and new designs based on the completion rates. The p-value of 0.261 is greater than the significance level of 0.05, suggesting that the observed differences in completion rates between the two designs are likely due to random chance rather than a significant effect of the design change. Therefore, we fail to reject the null hypothesis, and it appears that the new design does not have a statistically significant impact on completion rates compared to the old design."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929ed48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Data\n",
    "completion_rates = [old_completion_rate.mean(), new_completion_rate.mean()]\n",
    "labels = ['Old Design', 'New Design']\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(labels, completion_rates, color=['blue', 'orange'])\n",
    "plt.title('Completion Rates by Design')\n",
    "plt.xlabel('Design')\n",
    "plt.ylabel('Completion Rate')\n",
    "plt.ylim(0, 1)  # Set y-axis limit from 0 to 1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488b8edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convert time-related columns to datetime data type\n",
    "control_group_with_steps['start_time'] = pd.to_datetime(control_group_with_steps['start_time'])\n",
    "control_group_with_steps['end_time'] = pd.to_datetime(control_group_with_steps['end_time'])\n",
    "\n",
    "# Calculate duration in minutes\n",
    "control_group_with_steps['duration_minutes'] = (control_group_with_steps['end_time'] - control_group_with_steps['start_time']).dt.total_seconds() / 60\n",
    "\n",
    "# Display the DataFrame with the new duration column\n",
    "print(control_group_with_steps[['start_time', 'end_time', 'duration_minutes']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced69a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convert time-related columns to datetime data type\n",
    "test_group_with_steps['start_time'] = pd.to_datetime(test_group_with_steps['start_time'])\n",
    "test_group_with_steps['end_time'] = pd.to_datetime(test_group_with_steps['end_time'])\n",
    "\n",
    "# Calculate duration in minutes\n",
    "test_group_with_steps['duration_minutes'] = (test_group_with_steps['end_time'] - test_group_with_steps['start_time']).dt.total_seconds() / 60\n",
    "\n",
    "# Display the DataFrame with the new duration column\n",
    "print(test_group_with_steps[['start_time', 'end_time', 'duration_minutes']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f56ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "\n",
    "def hypothesis_test(control_data, test_data, alpha=0.05):\n",
    "    # Step 1: Formulate Hypotheses\n",
    "    # Null Hypothesis (H0): μ_test = μ_control\n",
    "    # Alternative Hypothesis (H1): μ_test ≠ μ_control\n",
    "    \n",
    "    # Step 2: Choose Significance Level (Alpha)\n",
    "    # Alpha is already provided as a parameter\n",
    "    \n",
    "    # Step 3: Calculate Test Statistic\n",
    "    t_statistic, p_value = stats.ttest_ind(control_data, test_data)\n",
    "    \n",
    "    # Step 4: Determine Degrees of Freedom\n",
    "    df = len(control_data) + len(test_data) - 2\n",
    "    \n",
    "    # Step 5: Calculate p-value\n",
    "    # For a two-tailed test, multiply the p-value by 2\n",
    "    p_value *= 2\n",
    "    \n",
    "    # Step 6: Make Decision\n",
    "    if p_value < alpha:\n",
    "        decision = \"Reject the null hypothesis\"\n",
    "    else:\n",
    "        decision = \"Fail to reject the null hypothesis\"\n",
    "    \n",
    "    return t_statistic, df, p_value, decision\n",
    "\n",
    "# Example usage:\n",
    "control_data = control_group_with_steps['duration_minutes']\n",
    "test_data = test_group_with_steps['duration_minutes']\n",
    "\n",
    "t_statistic, df, p_value, decision = hypothesis_test(control_data, test_data)\n",
    "\n",
    "print(\"Test Statistic:\", t_statistic)\n",
    "print(\"Degrees of Freedom:\", df)\n",
    "print(\"p-value:\", p_value)\n",
    "print(\"Decision:\", decision)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee678fa",
   "metadata": {},
   "source": [
    "Based on the provided data and the hypothesis test results:\n",
    "\n",
    "- Test Statistic: 2.8868\n",
    "  - The test statistic measures the difference between the mean durations of `test_group_with_steps` and `control_group_with_steps` in terms of standard error units.\n",
    "\n",
    "- Degrees of Freedom: 50485\n",
    "  - The degrees of freedom represent the number of independent pieces of information available in the data. In this case, it is determined by the sample sizes of both groups.\n",
    "\n",
    "- p-value: 0.0078\n",
    "  - The p-value is the probability of observing a test statistic as extreme as, or more extreme than, the one calculated from the sample data if the null hypothesis were true. Here, the p-value is less than the significance level (α = 0.05), indicating statistical significance.\n",
    "\n",
    "- Decision: Reject the null hypothesis\n",
    "  - Since the p-value is less than the chosen significance level, we reject the null hypothesis. This suggests that there is a statistically significant difference between the mean durations of `test_group_with_steps` and `control_group_with_steps`. Therefore, we can conclude that there is evidence to support the alternative hypothesis, indicating that the mean duration in `test_group_with_steps` is not equal to the mean duration in `control_group_with_steps`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73685d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get duration data for each group\n",
    "control_data = control_group_with_steps['duration_minutes']\n",
    "test_data = test_group_with_steps['duration_minutes']\n",
    "\n",
    "# Create a box plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.boxplot([control_data, test_data], labels=['Control Group', 'Test Group'])\n",
    "plt.title('Duration Comparison between Test Group and Control Group')\n",
    "plt.xlabel('Group')\n",
    "plt.ylabel('Duration (minutes)')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8de620",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "\n",
    "def hypothesis_test(observed_increase, alpha=0.05):\n",
    "    # Step 1: Formulate Hypotheses\n",
    "    # Null Hypothesis (H0): There is no difference in completion rates\n",
    "    # Alternative Hypothesis (H1): There is a difference in completion rates\n",
    "    \n",
    "    # Step 2: Choose Significance Level (Alpha)\n",
    "    \n",
    "    # Step 3: Calculate p-value\n",
    "    # The p-value can be calculated based on the observed increase\n",
    "    \n",
    "    # Step 4: Make Decision\n",
    "    if observed_increase < 0:\n",
    "        p_value = stats.norm.cdf(observed_increase)\n",
    "    else:\n",
    "        p_value = 1 - stats.norm.cdf(observed_increase)\n",
    "    \n",
    "    if p_value < alpha:\n",
    "        decision = \"Reject the null hypothesis\"\n",
    "    else:\n",
    "        decision = \"Fail to reject the null hypothesis\"\n",
    "    \n",
    "    return p_value, decision\n",
    "\n",
    "# Example usage:\n",
    "observed_increase = 0.02  # Example observed increase in completion rate\n",
    "alpha = 0.05\n",
    "\n",
    "p_value, decision = hypothesis_test(observed_increase, alpha)\n",
    "print(\"p-value:\", p_value)\n",
    "print(\"Decision:\", decision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f4f213",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "\n",
    "# Step 1: Formulate Hypotheses\n",
    "# Null Hypothesis (H0): There is no difference in the error rates between the control and test groups\n",
    "# Alternative Hypothesis (H1): There is a difference in the error rates between the control and test groups\n",
    "\n",
    "# Step 2: Choose Significance Level (Alpha)\n",
    "alpha = 0.05\n",
    "\n",
    "# Step 3: Calculate Test Statistic\n",
    "# Use chi-square test for independence\n",
    "contingency_table = pd.crosstab([control_group_with_steps['end_step'] != 'confirm'], [test_group_with_steps['end_step'] != 'confirm'])\n",
    "chi2_stat, p_value, _, _ = stats.chi2_contingency(contingency_table)\n",
    "\n",
    "# Step 4: Determine Degrees of Freedom\n",
    "degrees_of_freedom = contingency_table.values.size - sum(contingency_table.shape) + contingency_table.shape[0] - 1\n",
    "\n",
    "# Step 5: Calculate p-value\n",
    "\n",
    "# Step 6: Make Decision\n",
    "if p_value < alpha:\n",
    "    decision = \"Reject the null hypothesis\"\n",
    "else:\n",
    "    decision = \"Fail to reject the null hypothesis\"\n",
    "\n",
    "print(\"Chi-Square Statistic:\", chi2_stat)\n",
    "print(\"Degrees of Freedom:\", degrees_of_freedom)\n",
    "print(\"p-value:\", p_value)\n",
    "print(\"Decision:\", decision)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547d5bbc",
   "metadata": {},
   "source": [
    "The output of the chi-square test can be interpreted as follows:\n",
    "\n",
    "- **Chi-Square Statistic**: This value represents the test statistic calculated from the chi-square distribution. In this case, the value is approximately 1.26.\n",
    "\n",
    "- **Degrees of Freedom**: This value indicates the degrees of freedom associated with the chi-square distribution. It is calculated based on the dimensions of the contingency table used in the chi-square test. In this case, the degrees of freedom is 1.\n",
    "\n",
    "- **p-value**: The p-value is the probability of observing a chi-square statistic as extreme as, or more extreme than, the one calculated, assuming that the null hypothesis (no difference in error rates between the control and test groups) is true. In this case, the p-value is approximately 0.26.\n",
    "\n",
    "- **Decision**: Based on the chosen significance level (alpha = 0.05), the decision is made whether to reject or fail to reject the null hypothesis. If the p-value is less than the significance level, the null hypothesis is rejected, indicating that there is a significant difference in error rates between the control and test groups. Otherwise, if the p-value is greater than or equal to the significance level, the null hypothesis is not rejected, suggesting that there is insufficient evidence to conclude a difference in error rates between the groups. In this case, with a p-value of 0.26 (greater than 0.05), we fail to reject the null hypothesis. Therefore, there is not enough evidence to conclude a significant difference in error rates between the control and test groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46cc860",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Filter errors in control and test groups\n",
    "control_errors = control_group_with_steps[control_group_with_steps['end_step'] != 'confirm']\n",
    "test_errors = test_group_with_steps[test_group_with_steps['end_step'] != 'confirm']\n",
    "\n",
    "# Calculate error rates\n",
    "control_error_rate = control_errors.shape[0] / len(control_group_with_steps)\n",
    "test_error_rate = test_errors.shape[0] / len(test_group_with_steps)\n",
    "\n",
    "# Define groups and their error rates\n",
    "groups = ['Control Group', 'Test Group']\n",
    "error_rates = [control_error_rate, test_error_rate]\n",
    "\n",
    "# Create the bar plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(groups, error_rates, color=['blue', 'orange'])\n",
    "plt.title('Error Rates Comparison between Control and Test Groups')\n",
    "plt.xlabel('Group')\n",
    "plt.ylabel('Error Rate')\n",
    "plt.ylim(0, max(error_rates) * 1.2)  # Adjust y-axis limit for better visualization\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f9515f",
   "metadata": {},
   "outputs": [],
   "source": [
    "control_group_with_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb31562c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_group_with_steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862e3fde",
   "metadata": {},
   "source": [
    "Day 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82c62a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_group_merged"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6365d3",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming test_group_merged is your DataFrame\n",
    "# Convert date_time column to datetime data type\n",
    "test_group_merged['date_time'] = pd.to_datetime(test_group_merged['date_time'])\n",
    "\n",
    "# Calculate time duration for each step\n",
    "test_group_merged['duration'] = test_group_merged.groupby('client_id')['date_time'].diff()\n",
    "\n",
    "# Pivot the DataFrame to create new columns for each step with the average duration\n",
    "pivot_table = pd.pivot_table(test_group_merged, index='client_id', columns='process_step', values='duration', aggfunc='mean')\n",
    "\n",
    "# Display the result\n",
    "pivot_table\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3771325",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming test_group_merged is your DataFrame\n",
    "# Convert date_time column to datetime data type\n",
    "test_group_merged['date_time'] = pd.to_datetime(test_group_merged['date_time'])\n",
    "\n",
    "# Calculate time duration for each step\n",
    "test_group_merged['duration'] = test_group_merged.groupby('client_id')['date_time'].diff()\n",
    "\n",
    "# Convert duration to seconds\n",
    "test_group_merged['duration_seconds'] = test_group_merged['duration'].dt.total_seconds()\n",
    "\n",
    "# Convert duration to minutes\n",
    "test_group_merged['duration_minutes'] = test_group_merged['duration'].dt.total_seconds() / 60\n",
    "\n",
    "# Pivot the DataFrame to create new columns for each step with the average duration\n",
    "pivot_table = pd.pivot_table(test_group_merged, index='client_id', columns='process_step', values='duration_minutes', aggfunc='mean')\n",
    "\n",
    "# Display the result\n",
    "pivot_table\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0dd113",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming test_group_merged is your DataFrame\n",
    "# Convert date_time column to datetime data type\n",
    "test_group_merged['date_time'] = pd.to_datetime(test_group_merged['date_time'])\n",
    "\n",
    "# Sort the DataFrame by visit_id and date_time\n",
    "test_group_merged = test_group_merged.sort_values(by=['visit_id', 'date_time'])\n",
    "\n",
    "# Calculate the time spent on each step for each visit\n",
    "test_group_merged['time_diff'] = test_group_merged.groupby('visit_id')['date_time'].diff()\n",
    "\n",
    "# Drop rows where time_diff is null (for the first step of each visit)\n",
    "test_group_merged = test_group_merged.dropna(subset=['time_diff'])\n",
    "\n",
    "# Calculate the time spent in minutes\n",
    "test_group_merged['time_diff_minutes'] = test_group_merged['time_diff'].dt.total_seconds()\n",
    "\n",
    "# Group by visit_id and process_step, then calculate the average time spent on each step per visit\n",
    "average_time_per_step = test_group_merged.groupby(['visit_id', 'process_step'])['time_diff_minutes'].mean()\n",
    "\n",
    "# Display the result\n",
    "average_time_per_step\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61349dda",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming test_group_merged is your DataFrame\n",
    "# Convert date_time column to datetime data type\n",
    "test_group_merged['date_time'] = pd.to_datetime(test_group_merged['date_time'])\n",
    "\n",
    "# Sort the DataFrame by visit_id and date_time\n",
    "test_group_merged = test_group_merged.sort_values(by=['visit_id', 'date_time'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569b806b",
   "metadata": {},
   "source": [
    "test_group_merged['time_diff'] = test_group_merged.groupby('visit_id')['date_time'].diff()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5888ffbf",
   "metadata": {},
   "source": [
    "test_group_merged['time_diff'] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6336502",
   "metadata": {},
   "source": [
    "test_group_merged = test_group_merged.dropna(subset=['time_diff'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a62c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting 'date_time' column to datetime format\n",
    "test_group_merged['date_time'] = pd.to_datetime(test_group_merged['date_time'])\n",
    "# Splitting 'date_time' into 'date' and 'time' columns\n",
    "test_group_merged['date'] = test_group_merged['date_time'].dt.date\n",
    "test_group_merged['time'] = test_group_merged['date_time'].dt.time\n",
    "# Sorting the DataFrame by 'visit_id' and 'time'\n",
    "test_group_merged.sort_values(by=['visit_id', 'time'], inplace=True)\n",
    "# Removing duplicate steps, keeping only the first occurrence\n",
    "df_unique_steps = test_group_merged.drop_duplicates(subset=['visit_id', 'process_step'], keep='first')\n",
    "# Displaying the DataFrame with the new columns\n",
    "print(test_group_merged[['date', 'time']])\n",
    "test_group_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ebdd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the time difference for each step\n",
    "test_group_merged['time_diff'] = test_group_merged.groupby(['visit_id'])['date_time'].diff()\n",
    "# Grouping by 'process_step' and calculating the average time for each step\n",
    "avg_time_per_step = test_group_merged.groupby('process_step')['time_diff'].mean()\n",
    "# Printing the result\n",
    "print(avg_time_per_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ec9b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_group_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b870c09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting 'date_time' column to datetime format\n",
    "control_group_merged['date_time'] = pd.to_datetime(control_group_merged['date_time'])\n",
    "# Splitting 'date_time' into 'date' and 'time' columns\n",
    "control_group_merged['date'] = control_group_merged['date_time'].dt.date\n",
    "control_group_merged['time'] = control_group_merged['date_time'].dt.time\n",
    "# Sorting the DataFrame by 'visit_id' and 'time'\n",
    "control_group_merged.sort_values(by=['visit_id', 'time'], inplace=True)\n",
    "# Removing duplicate steps, keeping only the first occurrence\n",
    "df_unique_steps = control_group_merged.drop_duplicates(subset=['visit_id', 'process_step'], keep='first')\n",
    "# Displaying the DataFrame with the new columns\n",
    "print(control_group_merged[['date', 'time']])\n",
    "control_group_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3658e75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "control_group_merged['time_diff'] = control_group_merged.groupby('visit_id')['date_time'].diff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd81dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "control_group_merged = control_group_merged.dropna(subset=['time_diff'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd508b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "control_group_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c06edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the time difference for each step\n",
    "control_group_merged['time_diff'] = control_group_merged.groupby(['visit_id'])['date_time'].diff()\n",
    "# Grouping by 'process_step' and calculating the average time for each step\n",
    "avg_time_per_step = control_group_merged.groupby('process_step')['time_diff'].mean()\n",
    "# Printing the result\n",
    "print(avg_time_per_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5beb205d",
   "metadata": {},
   "outputs": [],
   "source": [
    "control_group_merged"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b04b2e",
   "metadata": {},
   "source": [
    "control:\n",
    "process_step\n",
    "confirm   0 days 00:02:08.178917240\n",
    "start     0 days 00:02:51.654397885\n",
    "step_1    0 days 00:00:53.973377525\n",
    "step_2    0 days 00:00:38.679543045\n",
    "step_3    0 days 00:01:32.305103583\n",
    "\n",
    "test:\n",
    "process_step\n",
    "confirm   0 days 00:02:05.367502259\n",
    "start     0 days 00:02:33.631839986\n",
    "step_1    0 days 00:00:48.127492226\n",
    "step_2    0 days 00:00:58.531975288\n",
    "step_3    0 days 00:01:36.516300257"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75cc5c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Data\n",
    "control_data = {\n",
    "    'confirm': 128.178917240,\n",
    "    'start': 171.654397885,\n",
    "    'step_1': 53.973377525,\n",
    "    'step_2': 38.679543045,\n",
    "    'step_3': 92.305103583\n",
    "}\n",
    "\n",
    "test_data = {\n",
    "    'confirm': 125.367502259,\n",
    "    'start': 153.631839986,\n",
    "    'step_1': 48.127492226,\n",
    "    'step_2': 58.531975288,\n",
    "    'step_3': 96.516300257\n",
    "}\n",
    "\n",
    "# Process steps\n",
    "process_steps = list(control_data.keys())\n",
    "\n",
    "# Time differences\n",
    "control_times = list(control_data.values())\n",
    "test_times = list(test_data.values())\n",
    "\n",
    "# Convert seconds to minutes for better visualization\n",
    "control_times = [time / 60 for time in control_times]\n",
    "test_times = [time / 60 for time in test_times]\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(process_steps, control_times, width=0.4, label='Control', color='blue', align='center')\n",
    "plt.bar(process_steps, test_times, width=0.4, label='Test', color='orange', align='edge')\n",
    "plt.xlabel('Process Step')\n",
    "plt.ylabel('Time (minutes)')\n",
    "plt.title('Time Difference between Control and Test Groups for Each Process Step')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e439905",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Data for control group\n",
    "control_data = {\n",
    "    'start': 2.51,\n",
    "    'step_1': 0.54,\n",
    "    'step_2': 0.39,\n",
    "    'step_3': 1.32,\n",
    "    'confirm': 2.08  # Convert timedelta to minutes\n",
    "}\n",
    "\n",
    "# Data for test group\n",
    "test_data = {\n",
    "    'start': 2.34,\n",
    "    'step_1': 0.48,\n",
    "    'step_2': 0.59,\n",
    "    'step_3': 1.37,\n",
    "    'confirm': 2.05  # Convert timedelta to minutes\n",
    "}\n",
    "\n",
    "# List of process steps sorted\n",
    "steps = sorted(control_data.keys())\n",
    "\n",
    "# Convert timedelta to minutes\n",
    "control_minutes = [control_data[step] for step in steps]\n",
    "test_minutes = [test_data[step] for step in steps]\n",
    "\n",
    "# Plotting the bar plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "bar_width = 0.35\n",
    "index = range(len(steps))\n",
    "plt.bar(index, control_minutes, bar_width, label='Control Group', color='darkblue')\n",
    "plt.bar([i + bar_width for i in index], test_minutes, bar_width, label='Test Group', color='orange')\n",
    "\n",
    "# Adding labels and title\n",
    "plt.xlabel('Process Step')\n",
    "plt.ylabel('Time (minutes)')\n",
    "plt.title('Time Difference on Each Step Between Test and Control Groups')\n",
    "plt.xticks([i + bar_width / 2 for i in index], steps)\n",
    "plt.legend()\n",
    "\n",
    "# Display the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52d5f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming test_group_merged is your DataFrame\n",
    "\n",
    "# Convert 'date_time' column to datetime\n",
    "test_group_merged['date_time'] = pd.to_datetime(test_group_merged['date_time'])\n",
    "\n",
    "# Filter rows where the process is completed on the same day\n",
    "same_day_data = test_group_merged[test_group_merged['date'] == test_group_merged['date_time'].dt.date]\n",
    "\n",
    "# Group by 'visit_id' to calculate total duration for each user\n",
    "grouped_data = same_day_data.groupby('visit_id')\n",
    "\n",
    "# Calculate total duration for each user\n",
    "total_duration_per_user = grouped_data['time_diff'].sum()  # Assuming 'time_diff' contains the duration of each step\n",
    "\n",
    "# Total duration across all process steps\n",
    "total_duration_across_all_users = total_duration_per_user.sum()\n",
    "\n",
    "# Display total duration\n",
    "print(\"Total duration across all users and steps:\", total_duration_across_all_users)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6d8101",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming test_group_merged is your DataFrame\n",
    "\n",
    "# Convert 'date_time' column to datetime\n",
    "test_group_merged['date_time'] = pd.to_datetime(test_group_merged['date_time'])\n",
    "\n",
    "# Sort the DataFrame by 'visit_id' and 'date_time'\n",
    "sorted_data = test_group_merged.sort_values(by=['visit_id', 'date_time'])\n",
    "\n",
    "# Group by 'visit_id' to count the number of 'confirm' steps for each user\n",
    "confirm_counts = sorted_data[sorted_data['process_step'] == 'confirm'].groupby('visit_id').size()\n",
    "\n",
    "# Filter the users who have only one 'confirm' step (completed within one day)\n",
    "clean_data = sorted_data[sorted_data['visit_id'].isin(confirm_counts[confirm_counts == 1].index)]\n",
    "\n",
    "# Display the clean data\n",
    "print(clean_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba49b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming you have already loaded your dataframes test_group_merged and control_group_merged\n",
    "\n",
    "# Concatenate the dataframes along the rows\n",
    "concatenated_df = pd.concat([test_group_merged, control_group_merged])\n",
    "\n",
    "# Save the concatenated dataframe to a CSV file\n",
    "concatenated_df.to_csv('concatenated_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b158d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc60eae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming you have already loaded your test_group_merged DataFrame\n",
    "\n",
    "# Add a new column 'Variation' with value 'Test'\n",
    "test_group_merged['Variation'] = 'Test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b422f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming you have already loaded your test_group_merged DataFrame\n",
    "\n",
    "# Add a new column 'Variation' with value 'Test'\n",
    "control_group_merged['Variation'] = 'Control'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce231512",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming you have already loaded your dataframes test_group_merged and control_group_merged\n",
    "\n",
    "# Concatenate the dataframes along the rows\n",
    "concatenated_df = pd.concat([test_group_merged, control_group_merged])\n",
    "\n",
    "# Save the concatenated dataframe to a CSV file\n",
    "concatenated_df.to_csv('concatenated_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45ea274",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Converting 'date_time' column to datetime format\n",
    "control_group_merged['date_time'] = pd.to_datetime(control_group_merged['date_time'])\n",
    "\n",
    "# Splitting 'date_time' into 'date' and 'time' columns\n",
    "control_group_merged['date'] = control_group_merged['date_time'].dt.date\n",
    "control_group_merged['time'] = control_group_merged['date_time'].dt.time\n",
    "\n",
    "# Sorting the DataFrame by 'visit_id' and 'time'\n",
    "control_group_merged.sort_values(by=['visit_id', 'time'], inplace=True)\n",
    "\n",
    "# Removing duplicate steps, keeping only the first occurrence\n",
    "df_unique_steps = control_group_merged.drop_duplicates(subset=['visit_id', 'process_step'], keep='first')\n",
    "\n",
    "# Calculate time spent on each step for each visit\n",
    "control_group_merged['time_spent'] = control_group_merged.groupby('visit_id')['date_time'].diff()\n",
    "\n",
    "# Displaying the DataFrame with the new column\n",
    "print(control_group_merged[['date', 'time', 'time_spent']])\n",
    "\n",
    "# Display the original DataFrame with the new column\n",
    "control_group_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd88703",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Converting 'date_time' column to datetime format\n",
    "control_group_merged['date_time'] = pd.to_datetime(control_group_merged['date_time'])\n",
    "\n",
    "# Splitting 'date_time' into 'date' and 'time' columns\n",
    "control_group_merged['date'] = control_group_merged['date_time'].dt.date\n",
    "control_group_merged['time'] = control_group_merged['date_time'].dt.time\n",
    "\n",
    "# Sorting the DataFrame by 'visit_id' and 'time'\n",
    "control_group_merged.sort_values(by=['visit_id', 'time'], inplace=True)\n",
    "\n",
    "# Removing duplicate steps, keeping only the first occurrence\n",
    "df_unique_steps = control_group_merged.drop_duplicates(subset=['visit_id', 'process_step'], keep='first')\n",
    "\n",
    "# Calculating the time spent on each step for each visit\n",
    "control_group_merged['time_diff'] = control_group_merged.groupby('visit_id')['date_time'].diff()\n",
    "\n",
    "# Converting the time difference to minutes\n",
    "control_group_merged['time_diff_minutes'] = control_group_merged['time_diff'].dt.total_seconds()\n",
    "\n",
    "# Displaying the DataFrame with the new column\n",
    "print(control_group_merged[['date', 'time', 'time_diff_minutes']])\n",
    "\n",
    "# Displaying the updated DataFrame\n",
    "print(control_group_merged)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1457aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
